\chapter{Numerical Sequences and Series}

\Exercise1 Prove that convergence of $\{s_n\}$ implies convergence of
$\{\abs{s_n}\}$. Is the converse true?
\begin{proof}
  Suppose $\{s_n\}$ converges to $s$ for some complex sequence
  $\{s_n\}$ and $s\in C$. Let $\varepsilon > 0$ be arbitrary. Then we
  may find $N$ such that $\abs{s_n - s} < \varepsilon$ for all
  $n\geq N$. Then, by Exercise~\ref{exercise-abs-abs-x-minus-abs-y} we
  have
  \begin{equation*}
    \abs{\abs{s_n} - \abs{s}} \leq \abs{s_n - s} < \varepsilon
    \quad\text{for each $n\geq N$.}
  \end{equation*}
  Hence $\{\abs{s_n}\}$ converges to $\abs{s}$.

  Note that the converse is {\em not} necessarily true. For example
  the real sequence $\{a_n\}$ given by $a_n = (-1)^n$ does not
  converge even though $\{\abs{a_n}\}$ converges to $1$.
\end{proof}

\Exercise2 Calculate $\displaystyle\lim_{n\to\infty}(\sqrt{n^2+n}-n)$.
\begin{solution}
  We have
  \begin{align*}
    \lim_{n\to\infty}(\sqrt{n^2+n}-n)
    &= \lim_{n\to\infty}\frac{(\sqrt{n^2+n}-n)(\sqrt{n^2+n}+n)}
      {\sqrt{n^2+n}+n} \\[3pt]
    &= \lim_{n\to\infty}\frac{n}{\sqrt{n^2+n}+n} \\[3pt]
    &= \lim_{n\to\infty}\frac1{\sqrt{1+\frac{1}n}+1} \\
    &= \frac12. \qedhere
  \end{align*}
\end{solution}

\Exercise3 If $s_1 = \sqrt2$, and
\begin{equation*}
  s_{n+1} = \sqrt{2 + \sqrt{s_n}}
  \qquad\text{($n=1,2,3,\dots$),}
\end{equation*}
prove that $\{s_n\}$ converges, and that $s_n<2$ for
$n = 1,2,3,\dots$.
\begin{proof}
  We will show by induction on $n$ that $\{s_n\}$ is a strictly
  increasing sequence that is bounded above by $2$. Certainly
  $\sqrt2 < \sqrt{2 + \sqrt2} < 2$, so the base case is
  satisfied. Suppose $s_n < s_{n+1} < 2$ for a positive integer
  $n$. Then
  \begin{equation*}
    s_{n+2} = \sqrt{2 + \sqrt{s_{n+1}}} > \sqrt{2 + \sqrt{s_n}} = s_{n+1}
  \end{equation*}
  and
  \begin{equation*}
    s_{n+2} = \sqrt{2 + \sqrt{s_{n+1}}}
    < \sqrt{2 + \sqrt{2}} < \sqrt{4} = 2.
  \end{equation*}
  Therefore $s_{n+1} < s_{n+2} < 2$ and it follows that $\{s_n\}$ is
  monotonic and bounded, and hence must converge.
\end{proof}

\Exercise4 Find the upper and lower limits of the sequence $\{s_n\}$
defined by
\begin{equation*}
  s_1 = 0;\quad s_{2m} = \frac{s_{2m-1}}2;
  \quad s_{2m+1} = \frac12 + s_{2m}.
\end{equation*}
\begin{solution}
  $\{s_n\}$ is the sequence
  \begin{equation*}
    0, \frac12, \frac14, \frac34, \frac38, \frac78,
    \frac7{16}, \frac{15}{16}, \dots.
  \end{equation*}
  The odd terms of $\{s_n\}$ form the sequence
  \begin{equation*}
    0, \frac14, \frac38, \frac7{16}, \dots, \frac{2^{n-1}-1}{2^n}, \dots
  \end{equation*}
  while the even terms form the sequence
  \begin{equation*}
    \frac12, \frac34, \frac78, \frac{15}{16}, \dots,
    \frac{2^n-1}{2^n}, \dots.
  \end{equation*}
  So
  \begin{align*}
    \liminf_{n\to\infty}s_n
    &= \lim_{n\to\infty}\frac{2^{n-1} - 1}{2^n} \\
    &= \lim_{n\to\infty}\left(\frac12 - \frac1{2^n}\right) \\
    &= \frac12,
  \end{align*}
  and
  \begin{align*}
    \limsup_{n\to\infty}s_n
    &= \lim_{n\to\infty}\frac{2^{n} - 1}{2^n} \\
    &= \lim_{n\to\infty}\left(1 - \frac1{2^n}\right) \\
    &= 1. \qedhere
  \end{align*}
\end{solution}

\Exercise5 For any two real sequences $\{a_n\}$, $\{b_n\}$, prove that
\begin{equation*}
  \limsup_{n\to\infty}(a_n + b_n)
  \leq \limsup_{n\to\infty}a_n + \limsup_{n\to\infty}b_n,
\end{equation*}
provided the sum on the right is not of the form $\infty - \infty$.
\begin{proof}
  For each positive integer $n$, put $c_n = a_n + b_n$. Let
  \begin{equation*}
    \alpha = \limsup_{n\to\infty}a_n,
    \quad
    \beta = \limsup_{n\to\infty}b_n,
    \quad\text{and}\quad
    \gamma = \limsup_{n\to\infty}c_n.
  \end{equation*}
  If $\alpha = \infty$ and $\beta\neq-\infty$ then the result is
  clear, and the case where $\alpha = -\infty$ and $\beta\neq\infty$
  is similar.

  So suppose $\alpha$ and $\beta$ are both finite. Let $\{c_{n_i}\}$
  be a subsequence of $\{c_n\}$ that converges to $\gamma$. Now let
  $\{a_{n_{i_j}}\}$ be a subsequence of $\{a_{n_i}\}$ such that
  \begin{equation*}
    \lim_{j\to\infty}a_{n_{i_j}} = \limsup_{i\to\infty}a_{n_i}.
  \end{equation*}
  Now since $\{c_{n_{i_j}}\}$ is a subsequence of $\{c_{n_i}\}$, it
  converges to the same limit $\gamma$. Then
  \begin{equation*}
    \lim_{j\to\infty}b_{n_{i_j}} = \lim_{j\to\infty}(c_{n_{i_j}} - a_{n_{i_j}})
    = \lim_{j\to\infty}c_{n_{i_j}} - \lim_{j\to\infty}a_{n_{i_j}}
    = \gamma - \limsup_{i\to\infty}a_{n_i}.
  \end{equation*}
  Rearranging, we get
  \begin{equation*}
    \gamma = \limsup_{i\to\infty}a_{n_i} + \lim_{j\to\infty}b_{n_{i_j}}.
  \end{equation*}
  But
  \begin{equation*}
    \limsup_{i\to\infty}a_{n_i} \leq \alpha
    \quad\text{and}\quad
    \lim_{j\to\infty}b_{n_{i_j}} \leq \beta,
  \end{equation*}
  so
  \begin{equation*}
    \gamma \leq \alpha + \beta
  \end{equation*}
  and the proof is complete.
\end{proof}

\Exercise6 Investigate the behavior (convergence or divergence) of
$\sum a_n$ if
\begin{enumerate}
\item $a_n = \sqrt{n+1} - \sqrt{n}$
  \begin{solution}
    Let $s_n$ denote the $n$th partial sum of $\sum a_n$. A simple
    induction argument will show that $s_n = \sqrt{n+1} -
    \sqrt1$. Since $s_n\to\infty$ as $n\to\infty$, the series
    $\sum a_n$ diverges.
  \end{solution}
\item $\displaystyle a_n = \frac{\sqrt{n+1} - \sqrt{n}}n$
  \begin{solution}
    We have
    \begin{equation*}
      a_n = \frac{(\sqrt{n+1} - \sqrt{n})(\sqrt{n+1} + \sqrt{n})}
            {n(\sqrt{n+1} + \sqrt{n})}
      = \frac1{n\sqrt{n+1} + n\sqrt{n}}.
    \end{equation*}
    so
    \begin{equation*}
      a_n \leq \frac1{2n^{3/2}} < \frac1{n^{3/2}}.
    \end{equation*}
    So by comparison (Theorem~3.25) with the convergent series
    $\sum1/n^{3/2}$, we see that $\sum a_n$ converges.
  \end{solution}
\item $a_n = (\sqrt[n]{n} - 1)^n$
  \begin{solution}
    By Theorem~3.20 (c),
    \begin{equation*}
      \lim_{n\to\infty}\sqrt[n]{a_n} = \lim_{n\to\infty}(\sqrt[n]{n} - 1)
      = 1 - 1 = 0.
    \end{equation*}
    Therefore, by the root test (Theorem~3.33), the series $\sum a_n$
    converges.
  \end{solution}
\item $\displaystyle a_n = \frac1{1+z^n}$ for complex values of $z$
  \begin{solution}
    First note that, by Exercise~\ref{exercise-abs-abs-x-minus-abs-y},
    we have
    \begin{equation*}
      \abs{z^n + 1} = \abs{z^n - (-1)}
      \geq \abs{\abs{z^n} - \abs{-1}}
      = \abs{\abs{z}^n - 1}.
    \end{equation*}
    Then
    \begin{equation}
      \label{eq:abs-1-over-1-plus-z-n-inequality}
      \Abs{\frac1{1 + z^n}} \leq \frac1{\abs{\abs{z}^n - 1}}.
    \end{equation}

    Now suppose $\abs{z} > 1$. Then there is an integer $N$ such that
    $\abs{z}^n > 2$ for all $n\geq N$. That is,
    \begin{equation*}
      \frac1{\abs{z}^n - 1}\leq \frac2{\abs{z}^n}
      \quad\text{for $n\geq N$}.
    \end{equation*}
    Using this fact, \eqref{eq:abs-1-over-1-plus-z-n-inequality}
    becomes
    \begin{equation*}
      \Abs{\frac1{1 + z^n}} \leq \frac2{\abs{z}^n}
      \quad
      \text{for $n\geq N$}.
    \end{equation*}
    So by the comparison test with the convergent geometric series
    $\sum 2/\abs{z}^n$ we have that $\sum a_n$ also converges.

    In the case where $\abs{z} \leq 1$, it is easy to see that
    $a_n\not\to 0$ as $n\to\infty$, so $\sum a_n$ diverges.
  \end{solution}
\end{enumerate}

\Exercise7 Prove that the convergence of $\sum a_n$ implies the
convergence of
\begin{equation*}
  \sum\frac{\sqrt{a_n}}n,
\end{equation*}
if $a_n\geq0$.
\begin{proof}
  Since
  \begin{equation*}
    \left(\sqrt{a_n} + \frac1n\right)^2 \geq 0,
  \end{equation*}
  we may expand and rearrange to get
  \begin{equation}
    \label{eq:sqrt-a-n-inequality}
    \frac{\sqrt{a_n}}n \leq \frac12\left(a_n + \frac1{n^2}\right).
  \end{equation}
  Since $\sum a_n$ and $\sum1/n^2$ both converge, we know by
  Theorem~3.47 that their sum,
  \begin{equation*}
    \sum_{n=1}^\infty\left(a_n + \frac1{n^2}\right),
  \end{equation*}
  also converges. By the comparison test,
  \eqref{eq:sqrt-a-n-inequality} implies that $\sum\sqrt{a_n}/n$ must
  converge.
\end{proof}

\Exercise8 If $\sum a_n$ converges, and if $\{b_n\}$ is monotonic and
bounded, prove that $\sum a_nb_n$ converges.
\begin{proof}
  Let $A_n$ denote the $n$th partial sum of $\sum a_n$. That is,
  \begin{equation*}
    A_n = \sum_{k=1}^n a_k.
  \end{equation*}
  Suppose $\{A_n\}$ converges to $A$. We know that $\{b_n\}$ must
  converge, so set $b = \lim_{n\to\infty}b_n$. Then
  \begin{equation*}
    \lim_{n\to\infty}A_nb_n = Ab.
  \end{equation*}

  Now, since $\{A_n\}$ converges, it must be bounded, so we can find
  $M_0$ such that $\abs{A_n} < M_0$ for all $n$. We can also find
  $M_1$ such that $\abs{b_n} < M_1$ for all $n$. Take
  $M = \max(M_0, M_1)$. Then for any $\varepsilon>0$, we may find $N$
  such that
  \begin{equation*}
    \abs{A_nb_n - Ab} < \frac\varepsilon3
    \quad\text{and}\quad
    \abs{b_m - b_n} < \frac{\varepsilon}{3M}
    \quad\text{for all $m,n\geq N$}.
  \end{equation*}

  Since $\{b_n\}$ is monotonic, we have for all $q > p > N$ that
  \begin{align*}
    \Abs{\sum_{n=p}^{q-1} A_n(b_n - b_{n+1})}
    &\leq M\Abs{\sum_{n=p}^{q-1}(b_n - b_{n+1})} \\
    &= M\abs{b_p - b_q} < \frac{\varepsilon}3,
  \end{align*}
  and
  \begin{align*}
    \abs{A_qb_q - A_{p-1}b_p}
    &= \abs{A_qb_q - Ab + Ab - A_{p-1}b_p} \\
    &\leq \abs{A_qb_q - Ab} + \abs{A_{p-1}b_p - Ab} \\
    &< \frac\varepsilon3 + \frac\varepsilon3
    = \frac{2\varepsilon}3.
  \end{align*}

  Finally, using the partial summation formula from Theorem~3.41, we
  have
  \begin{align*}
    \Abs{\sum_{n=p}^q a_nb_n}
    &= \Abs{\sum_{n=p}^{q-1}A_n(b_n - b_{n+1}) + A_qb_q - A_{p-1}b_p} \\
    &\leq \Abs{\sum_{n=p}^{q-1}A_n(b_n - b_{n+1})} + \abs{A_qb_q - A_{p-1}b_p} \\
    &< \frac\varepsilon3 + \frac{2\varepsilon}3 \\
    &= \varepsilon.
  \end{align*}
  By the Cauchy criterion, $\sum a_nb_n$ converges.
\end{proof}

\Exercise9 Find the radius of convergence of each of the following
power series:
\begin{enumerate}
\item $\displaystyle\sum n^3z^n$
  \begin{solution}
    Using the ratio test, we have
    \begin{equation*}
      \lim_{n\to\infty}\Abs{\frac{(n+1)^3z^{n+1}}{n^3z^n}}
      = \lim_{n\to\infty}\frac{n+1}n\abs{z} = \abs{z}.
    \end{equation*}
    So the series converges when $\abs{z} < 1$. Therefore the radius
    of convergence is $R = 1$.
  \end{solution}
\item $\displaystyle\sum\frac{2^n}{n!}z^n$
  \begin{solution}
    Applying the ratio test again, we get,
    \begin{equation*}
      \lim_{n\to\infty}\Abs{\frac{2^{n+1}z^{n+1}n!}{2^nz^n(n+1)!}}
        = \lim_{n\to\infty}\frac2{n+1}\abs{z} = 0,
      \end{equation*}
      so $R = \infty$.
  \end{solution}
\item $\displaystyle\sum\frac{2^n}{n^2}z^n$
  \begin{solution}
    We have
    \begin{equation*}
      \lim_{n\to\infty}\Abs{\frac{2^{n+1}z^{n+1}n^2}{2^nz^n(n+1)^2}}
      = \lim_{n\to\infty}2\left(\frac{n}{n+1}\right)^2\abs{z}
      = 2\abs{z}
    \end{equation*}
    so $R = 1/2$.
  \end{solution}
\item $\displaystyle\sum\frac{n^3}{3^n}z^n$
  \begin{solution}
    Again,
    \begin{equation*}
      \lim_{n\to\infty}\Abs{\frac{(n+1)^3z^{n+1}3^n}{n^3z^n3^{n+1}}}
      = \lim_{n\to\infty}\frac13\left(\frac{n+1}n\right)^3\abs{z}
      = \frac13\abs{z},
    \end{equation*}
    so $R = 3$.
  \end{solution}
\end{enumerate}

\Exercise{10} Suppose that the coefficients of the power series
$\sum a_nz^n$ are integers, infinitely many of which are distinct from
zero. Prove that the radius of convergence is at most $1$.
\begin{proof}
  For contradiction, suppose that $\abs{z} > 1$ while the sum
  \begin{equation*}
    \sum_{n=0}^\infty a_nz^n
  \end{equation*}
  converges. Then we know that $a_nz^n\to0$. In particular if
  $\varepsilon = 1$, we may find an integer $N$ such that
  \begin{equation*}
    \abs{a_nz^n} < 1
    \quad\text{for all $n\geq N$}.
  \end{equation*}
  But since $a_n$ is an integer and is nonzero for infinitely many
  $n$, we may also find $n_0\geq N$ such that $a_{n_0}\geq1$. Then
  \begin{equation*}
    \abs{a_{n_0}z^{n_0}} = \abs{a_{n_0}}\abs{z^{n_0}} \geq 1.
  \end{equation*}
  This is a contradiction, so $\abs{z}$ cannot be greater than
  $1$. Therefore the radius of convergence is at most $1$.
\end{proof}

\Exercise{11} Suppose $a_n > 0$, $s_n = a_1 + \cdots + a_n$, and
$\sum a_n$ diverges.
\begin{enumerate}
\item Prove that $\displaystyle\sum\frac{a_n}{1 + a_n}$ diverges.
  \begin{proof}
    Suppose the sum converges. Then $a_n/(1 + a_n)\to0$, which implies
    that $a_n\to0$. So we can find a positive integer $N$ such that
    $a_n < 1$ for all $n\geq N$. Then for $n\geq N$ we have
    \begin{equation*}
      \frac{a_n}{1 + a_n} \geq \frac{a_n}{1 + 1} = \frac12a_n.
    \end{equation*}
    Then the series $\sum a_n/2$ and hence $\sum a_n$ must converge by
    the comparison test, a contradiction. Therefore
    $\sum a_n/(1 + a_n)$ must diverge.
  \end{proof}
\item Prove that
  \begin{equation*}
    \frac{a_{N+1}}{s_{N+1}} + \cdots
    + \frac{a_{N+k}}{s_{N+k}} \geq 1 - \frac{s_N}{s_{N+k}}
  \end{equation*}
  and deduce that $\displaystyle\sum\frac{a_n}{s_n}$ diverges.
  \begin{proof}
    Since $a_n > 0$, the sequence $\{s_n\}$ is monotonically
    increasing. Then for any positive integers $N$ and $k$, we have
    \begin{align*}
      \frac{a_{N+1}}{s_{N+1}} + \cdots
      + \frac{a_{N+k}}{s_{N+k}}
      &\geq \frac{a_{N+1}+\cdots+a_{N+k}}{s_{N+k}} \\
      &= \frac{s_{N+k} - s_N}{s_{N+k}} = 1 - \frac{s_N}{s_{N+k}}.
    \end{align*}
    This establishes the desired inequality.

    Now, since $s_n$ is monotonic, it cannot be bounded (for
    otherwise $\sum a_n$ would converge). So for any fixed $N>0$, we may
    find a positive integer $k$ so that
    \begin{equation*}
      s_{N+k} > 2s_N.
    \end{equation*}
    Then
    \begin{equation*}
      \sum_{n=1}^k\frac{a_{N+n}}{s_{N+n}}
      \geq 1 - \frac{s_N}{s_{N+k}} \geq 1 - \frac12 = \frac12.
    \end{equation*}
    Therefore $\sum a_n/s_n$ diverges by the Cauchy criterion.
  \end{proof}
\item Prove that
  \begin{equation*}
    \frac{a_n}{s_n^2}\leq\frac1{s_{n-1}} - \frac1{s_n}
  \end{equation*}
  and deduce that $\displaystyle\sum\frac{a_n}{s_n^2}$ converges.
  \begin{proof}
    For any $n$,
    \begin{equation}
      \label{eq:recip-s-n-minus-1-recip-s-n-ineq}
      \frac1{s_{n-1}}-\frac1{s_n} = \frac{s_n - s_{n-1}}{s_{n-1}s_n}
      = \frac{a_n}{s_{n-1}s_n} \geq \frac{a_n}{s_n^2}.
    \end{equation}
    Now notice that
    \begin{equation*}
      \sum_{n=2}^k\left(\frac1{s_{n-1}} - \frac1{s_n}\right)
      = \frac1{s_1} - \frac1{s_k}.
    \end{equation*}
    Since the right-hand side of this equation tends to $1/s_1$ as
    $k\to\infty$, we see that the sum on the left converges. By the
    comparison test with \eqref{eq:recip-s-n-minus-1-recip-s-n-ineq},
    it follows that $\sum a_n/s_n^2$ converges.
  \end{proof}
\item What can be said about
  \begin{equation*}
    \sum\frac{a_n}{1 + na_n}
    \quad\text{and}\quad
    \sum\frac{a_n}{1 + n^2a_n}?
  \end{equation*}
  \begin{solution}
    The series on the right must converge by comparison with the
    convergent series $\sum1/n^2$.

    However, the series on the left may converge or diverge depending
    on the nature of $\{a_n\}$. For example, if $a_n = 1$ for all $n$,
    then
    \begin{equation*}
      \frac{a_n}{1 + na_n} = \frac1{1 + n},
    \end{equation*}
    which is just the divergent harmonic series without the first
    term.

    On the other hand, if we set $a_n = 1$ when $n$ is a perfect
    square and $a_n = 1/n^2$ otherwise, then $\{a_n\}$ does diverge
    but $\sum a_n/(1 + na_n)$ converges, as we will now show. Let $P$
    be the set of perfect squares. Then
    \begin{align*}
      \sum_{n=1}^{m^2}\frac{a_n}{1 + na_n}
      &= \sum_{\substack{1\leq n\leq m^2 \\ n\in P}}\frac{a_n}{1 + na_n}
      + \sum_{\substack{1\leq n\leq m^2 \\ n\not\in P}}\frac{a_n}{1 + na_n} \\
      &= \sum_{n=1}^m\frac1{1 + n^2}
        + \sum_{\substack{1\leq n\leq m^2 \\ n\not\in P}}\frac1{n^2 + n} \\
      &\leq \sum_{n=1}^m\frac1{1 + n^2} + \sum_{n=1}^{m^2}\frac1{n^2 + n}.
    \end{align*}
    If we let $m\to\infty$, then the right-hand side converges, and it
    follows that $\sum a_n/(1 + na_n)$ also converges.
  \end{solution}
\end{enumerate}

\Exercise{12} Suppose $a_n > 0$ and $\sum a_n$ converges. Put
\begin{equation*}
  r_n = \sum_{m=n}^\infty a_m.
\end{equation*}
\begin{enumerate}
\item Prove that
  \begin{equation*}
    \frac{a_m}{r_m} + \cdots + \frac{a_n}{r_n} > 1 - \frac{r_n}{r_m}
  \end{equation*}
  if $m < n$, and deduce that $\sum\frac{a_n}{r_n}$ diverges.
  \begin{proof}
    Note that the sequence $\{r_n\}$ is strictly decreasing and
    bounded below by $0$. So if $m < n$, we have
    \begin{equation*}
      \frac{a_m}{r_m} + \cdots + \frac{a_n}{r_n}
      > \frac{a_m + \cdots + a_n}{r_m}
      = \frac{r_m - r_{n+1}}{r_m} > 1 - \frac{r_n}{r_m}.
    \end{equation*}
    Fix an $m > 0$. Since $r_n\to0$ we can always find $n > m$ so that
    $r_n < r_m/2$. Then $1 - r_n/r_m > 1/2$. So no matter how large
    $N$ is, it is possible to find $n>m\geq N$ such that
    \begin{equation*}
      \sum_{k=m}^n\frac{a_k}{r_k} > \frac12,
    \end{equation*}
    and therefore the series diverges by the Cauchy criterion.
  \end{proof}
\item Prove that
  \begin{equation*}
    \frac{a_n}{\sqrt{r_n}} < 2(\sqrt{r_n} - \sqrt{r_{n+1}})
  \end{equation*}
  and deduce that $\sum\frac{a_n}{\sqrt{r_n}}$ converges.
  \begin{proof}
    For any index $n$, we have
    \begin{align*}
      \frac{a_n}{\sqrt{r_n}}
      &= \frac{r_n - r_{n+1}}{\sqrt{r_n}} \\
      &= \frac{(\sqrt{r_n} + \sqrt{r_{n+1}})
        (\sqrt{r_n} - \sqrt{r_{n+1}})}{\sqrt{r_n}} \\
      &= \left(1 + \frac{\sqrt{r_{n+1}}}{\sqrt{r_n}}\right)
        (\sqrt{r_n} - \sqrt{r_{n+1}}) \\
      &< 2(\sqrt{r_n} - \sqrt{r_{n+1}}).
    \end{align*}
    Now observe that, since $r_n\to0$, we have
    \begin{align*}
      \sum_{n=1}^\infty(\sqrt{r_n} - \sqrt{r_{n+1}})
      &= \lim_{m\to\infty}\sum_{n=1}^m(\sqrt{r_n} - \sqrt{r_{n+1}}) \\
      &= \lim_{m\to\infty}(\sqrt{r_1} - \sqrt{r_{m+1}}) \\
      &= \sqrt{r_1}.
    \end{align*}
    This series converges, so by the comparison test,
    $\sum a_n/\sqrt{r_n}$ also converges.
  \end{proof}
\end{enumerate}

\Exercise{13} Prove that the Cauchy product of two absolutely
convergent series converges absolutely.
\begin{proof}
  We will imitate the proof of Theorem~3.50. Suppose $\sum a_n$ and
  $\sum b_n$ both converge absolutely, define
  \begin{equation*}
    c_n = \sum_{k=0}^n a_kb_{n-k}
  \end{equation*}
  for each positive integer $n$, and set
  \begin{equation*}
    \sum_{n=0}^\infty\abs{a_n} = A
    \quad\text{and}\quad
    \sum_{n=0}^\infty\abs{b_n} = B.
  \end{equation*}
  Further, put
  \begin{equation*}
    A_n = \sum_{k=0}^n\abs{a_k},
    \quad
    B_n = \sum_{k=0}^n\abs{b_k},
    \quad
    C_n = \sum_{k=0}^n\abs{c_k},
    \quad\text{and}\quad
    \beta_n = B_n - B.
  \end{equation*}
  Then
  \begin{align*}
    C_n &= \abs{a_0b_0} + \abs{a_0b_1 + a_1b_0}
          + \cdots + \abs{a_0b_n + a_1b_{n-1} + \cdots + a_nb_0} \\
        &\leq \abs{a_0}\abs{b_0} + (\abs{a_0}\abs{b_1}
          + \abs{a_1}\abs{b_0}) + \cdots + (\abs{a_0}\abs{b_n}
          + \cdots + \abs{a_n}\abs{b_0}) \\
        &= \abs{a_0}B_n + \abs{a_1}B_{n-1} + \cdots + \abs{a_n}B_0 \\
        &= \abs{a_0}(B + \beta_n) + \abs{a_1}(B + \beta_{n-1})
          + \cdots + \abs{a_n}(B + \beta_0) \\
        &= A_nB + \abs{a_0}\beta_n + \abs{a_1}\beta_{n-1}
          + \cdots + \abs{a_n}\beta_0.
  \end{align*}
  Now set
  \begin{equation*}
    \gamma_n = \abs{a_0}\beta_n + \abs{a_1}\beta_{n-1}
    + \cdots + \abs{a_n}\beta_0.
  \end{equation*}
  Let $\varepsilon > 0$ be given and note that $\beta_n\to0$. So we
  can choose $N$ such that $\abs{\beta_n}\leq\varepsilon$ for
  $n\geq N$. Then
  \begin{align*}
    \abs{\gamma_n}
    &\leq \abs{\beta_0\abs{a_n} + \cdots + \beta_N\abs{a_{n-N}}}
      + \abs{\beta_{N+1}\abs{a_{n-N-1}} + \cdots + \beta_n\abs{a_0}} \\
    &\leq \abs{\beta_0\abs{a_n} + \cdots + \beta_N\abs{a_{n-N}}}
      + \varepsilon A.
  \end{align*}
  Keeping $N$ fixed and letting $n\to\infty$ gives
  \begin{equation*}
    \limsup_{n\to\infty}\,\abs{\gamma_n}\leq\varepsilon A,
  \end{equation*}
  since $\abs{a_k}\to0$ as $k\to\infty$. Since $\varepsilon$ was
  arbitrary, this shows that $\lim_{n\to\infty}\gamma_n = 0$.

  Returning to $C_n$, we have
  \begin{equation*}
    C_n \leq A_nB + \gamma_n.
  \end{equation*}
  Since $A_nB\to AB$ and $\gamma_n\to0$, we see that the sequence
  $\{C_n\}$ is bounded above. But $\{C_n\}$ is a sequence of partial
  sums for a series in which every term is nonnegative, so the
  sequence $\{C_n\}$ is also monotonically increasing. Therefore
  $\sum\abs{c_n}$ converges, so $\sum c_n$ converges absolutely.
\end{proof}

% \Exercise{14} If $\{s_n\}$ is a complex sequence, define its
% arithmetic means $\sigma_n$ by
% \begin{equation*}
%   \sigma_n = \frac{s_0 + s_1 + \cdots + s_n}{n + 1}
%   \quad
%   (n = 0, 1, 2, \dots).
% \end{equation*}
% \begin{enumerate}
% \item If $\lim s_n = s$, prove that $\lim\sigma_n = s$.
%   \begin{proof}
%     Let $\lim s_n = s$. For all nonnegative integers $n$ we have
%     \begin{align}
%       \begin{split}
%         \label{eq:sum-abs-s-0-minus-s}
%         \abs{\sigma_n - s}
%         &= \Abs{\frac{s_0 + s_1 + \cdots + s_n}{n+1} - s} \\
%         &= \frac1{n + 1}\abs{s_0 + s_1 + \cdots + s_n - (n + 1)s} \\
%         &= \frac1{n + 1}\abs{(s_0 - s) + (s_1 - s)
%           + \cdots + (s_n - s)} \\
%         &\leq \frac1{n + 1}\left(\abs{s_0 - s} + \abs{s_1 - s}
%           + \cdots + \abs{s_n - s}\right).
%       \end{split}
%     \end{align}
%     Since $\{s_n\}$ converges it must be bounded (Theorem~3.2~(c)), so
%     let $M > \abs{s_n}$ for all nonnegative integers $n$. Then
%     \begin{equation}
%       \label{eq:bound-for-sn-minus-s}
%       \abs{s_n - s} \leq \abs{s_n} + \abs{s} < M + \abs{s}
%       \quad\text{for all $n\geq0$}.
%     \end{equation}

%     Now let $\varepsilon > 0$ be arbitrary. We may find a positive
%     integer $N$ such that
%     \begin{equation}
%       \label{eq:abs-sn-minus-s}
%       \abs{s_n - s} < \frac\varepsilon2
%       \quad\text{for all $n\geq N$}.
%     \end{equation}
%     Set
%     \begin{equation}
%       \label{eq:define-N0}
%       N_0 = \frac{2(N + 1)(M + \abs{s})}\varepsilon.
%     \end{equation}
%     Using \eqref{eq:sum-abs-s-0-minus-s},
%     \eqref{eq:bound-for-sn-minus-s}, \eqref{eq:abs-sn-minus-s}, and
%     \eqref{eq:define-N0}, we have for all $n\geq N_0$ that
%     \begin{align*}
%       \abs{\sigma_n - s}
%       &\leq \frac1{n+1}\sum_{i=0}^N\abs{s_i-s}
%         + \frac1{n+1}\sum_{i=N+1}^n\abs{s_i-s} \\
%       &\leq \frac{\varepsilon(N+1)(M + \abs{s})}{2(N+1)(M + \abs{s})}
%         + \frac1{n+1}\left(\frac\varepsilon2\right)(n - N) \\
%       &\leq \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon.
%     \end{align*}
%     Therefore $\lim_{n\to\infty}\sigma_n = s$.
%   \end{proof}
% \item Construct a sequence $\{s_n\}$ which does not converge, although
%   $\lim\sigma_n = 0$.
%   \begin{solution}
%     For each positive integer $n$, set $s_n = (-1)^n$. Obviously
%     $\{s_n\}$ does not converge, but
%     \begin{equation*}
%       \abs{\sigma_n} \leq \frac1{n+1}
%     \end{equation*}
%     so $\sigma_n\to0$.
%   \end{solution}
% \item Can it happen that $s_n > 0$ for all $n$ and that
%   $\limsup s_n = \infty$, although $\lim\sigma_n = 0$?
%   \begin{solution}
%     For each $n\geq0$, define
%     \begin{equation*}
%       s_n =
%       \begin{cases}
%         \sqrt{n} & \text{if $n = k^2$ for some $k\in Z$}, \\
%         1/n & \text{otherwise}.
%       \end{cases}
%     \end{equation*}
%     Then
%     \begin{equation*}
%       \sigma_{n^2} \leq \sum_{k=1}^{n^2}\frac1k + \sum_{k=1}^nk
%     \end{equation*}
%   \end{solution}
% \item Put $a_n = s_n - s_{n-1}$, for $n\geq1$. Show that
%   \begin{equation*}
%     s_n - \sigma_n = \frac1{n+1}\sum_{k=1}^nka_k.
%   \end{equation*}
%   Assume that $\lim(na_n) = 0$ and that $\{\sigma_n\}$
%   converges. Prove that $\{s_n\}$ converges.
% \item Derive the last conclusion from a weaker hypothesis: Assume
%   $M < \infty$, $\abs{na_n}\leq M$ for all $n$, and
%   $\lim\sigma_n = \sigma$. Prove that $\lim s_n = \sigma$, by
%   completing the following outline:

%   If $m < n$, then
%   \begin{equation*}
%     s_n - \sigma_n = \frac{m+1}{n-m}(\sigma_n - \sigma_m)
%     + \frac1{n-m}\sum_{i=m+1}^n(s_n - s_i).
%   \end{equation*}
%   For these $i$,
%   \begin{equation*}
%     \abs{s_n - s_i} \leq \frac{(n-i)M}{i+1} \leq \frac{(n-m-1)M}{m+2}.
%   \end{equation*}

%   Fix $\varepsilon > 0$ and associate with each $n$ the integer $m$
%   that satisfies
%   \begin{equation*}
%     m \leq \frac{n - \varepsilon}{1 + \varepsilon} < m + 1.
%   \end{equation*}
%   Then $(m+1)/(n-m)\leq1/\varepsilon$ and
%   $\abs{s_n-s_i} < M\varepsilon$. Hence
%   \begin{equation*}
%     \limsup_{n\to\infty}\abs{s_n - \sigma} \leq M\varepsilon.
%   \end{equation*}
%   Since $\varepsilon$ was arbitrary, $\lim s_n = \sigma$.
% \end{enumerate}

\Exercise{15} Definition~3.21 can be extended to the case in which the
$a_n$ lie in some fixed $R^k$. Absolute convergence is defined as
convergence of $\sum\abs{\vec{a}_n}$. Show that Theorems 3.22, 3.23,
3.25(a), 3.33, 3.34, 3.42, 3.45, 3.47, and 3.55 are true in this more
general setting.
\begin{thm}
  $\sum\vec{a}_n$ converges if and only if for every $\varepsilon > 0$
  there is an integer $N$ such that
  \begin{equation*}
    \Abs{\sum_{k=n}^m\vec{a}_k}\leq\varepsilon
  \end{equation*}
  if $m\geq n\geq N$.
\end{thm}
\begin{proof}
  Consider the sequence $\{\vec{a}_n\}$ in $R^t$ given by
  \begin{equation*}
    \vec{a}_n = (a_{1,n}, a_{2,n}, \dots, a_{t,n}).
  \end{equation*}

  Combining Theorem~3.4 with the original Theorem~3.22, we have that
  $\sum\vec{a}_n$ converges if and only if for each $\varepsilon_i>0$
  ($i = 1,2,\dots,t$) there is $N_i$ such that
  \begin{equation}
    \label{eq:partial-sum-of-vectors-is-cauchy}
    \Abs{\sum_{k=n}^ma_{i,k}} \leq \varepsilon_i
    \quad\text{for all $m\geq n\geq N_i$}.
  \end{equation}

  Suppose \eqref{eq:partial-sum-of-vectors-is-cauchy} holds and let
  $\varepsilon>0$. For each $i$, set
  $\varepsilon_i = \varepsilon/\sqrt{t}$ and find the corresponding
  $N_i$. Then if $J = \max(N_1,N_2,\dots,N_t)$ we have for all
  $m\geq n\geq J$ that
  \begin{equation*}
    \Abs{\sum_{k=n}^m\vec{a}_k}
    = \sqrt{\left(\sum_{k=n}^ma_{1,k}\right)^2
      + \cdots
      + \left(\sum_{k=n}^ma_{t,k}\right)^2}
    \leq \sqrt{\varepsilon^2} = \varepsilon.
  \end{equation*}

  Conversely, let each $\varepsilon_i>0$ be given and choose $N$ such that
  \begin{equation*}
    \Abs{\sum_{k=n}^m\vec{a}_k}\leq\min(\varepsilon_1,
    \dots,\varepsilon_t)
    \quad\text{for $m\geq n\geq N$}.
  \end{equation*}
  Then for each $i$ and for all $m\geq n\geq N$ we have
  \begin{equation*}
    \Abs{\sum_{k=n}^ma_{i,k}}
    = \sqrt{\left(\sum_{k=n}^ma_{i,k}\right)^2}
    \leq \sqrt{\left(\sum_{k=n}^ma_{1,k}\right)^2
      +\cdots+
      \left(\sum_{k=n}^ma_{t,k}\right)^2} \leq \varepsilon_i.
  \end{equation*}
  Therefore \eqref{eq:partial-sum-of-vectors-is-cauchy} holds for each
  $i$ and the proof is complete.
\end{proof}
\begin{thm}
  If $\sum\vec{a}_n$ converges, then $\lim_{n\to\infty}\vec{a}_n = \vec0$.
\end{thm}
\begin{proof}
  This is immediate from Theorem~3.4 combined with the original
  Theorem~3.23.
\end{proof}
\begin{thm}
  If $\abs{\vec{a}_n}\leq c_n$ for $n\geq N_0$, where $N_0$ is
  some fixed integer, and if $\sum c_n$ converges, then
  $\sum\vec{a}_n$ converges.
\end{thm}
\begin{proof}
  The proof is the same as the original proof of Theorem~3.25: given
  $\varepsilon>0$, by the Cauchy criterion there exists $N\geq N_0$
  such that $m\geq n\geq N$ implies
  \begin{equation*}
    \sum_{k=n}^m c_k\leq\varepsilon.
  \end{equation*}
  By the triangle inequality,
  \begin{equation*}
    \Abs{\sum_{k=n}^m\vec{a}_k}\leq\sum_{k=n}^m\abs{\vec{a}_k}
    \leq\sum_{k=n}^mc_k\leq\varepsilon. \qedhere
  \end{equation*}
\end{proof}
\begin{thm}[Root Test]
  Given $\sum\vec{a}_n$, put
  \begin{equation*}
    \alpha = \limsup_{n\to\infty}\sqrt[n]{\abs{\vec{a}_n}}.
  \end{equation*}
  Then
  \begin{enumerate}
  \item if $\alpha < 1$, $\sum\vec{a}_n$ converges;
  \item if $\alpha > 1$, $\sum\vec{a}_n$ diverges;
  \item if $\alpha = 1$, the test gives no information.
  \end{enumerate}
\end{thm}
\begin{proof}
  Again, this proof is an easy generalization of the original proof of
  Theorem~3.33:

  If $\alpha < 1$, choose $\beta$ so that
  $\alpha < \beta < 1$. Then we can find an integer $N$ such that
  \begin{equation*}
    \sqrt[n]{\abs{\vec{a}_n}} < \beta
    \quad\text{for $n\geq N$}.
  \end{equation*}
  Then $\abs{\vec{a}_n}<\beta^n$ for all $n\geq N$, and the
  convergence of $\sum\vec{a}_n$ follows from the comparison test,
  since $\sum\beta^n$ converges.

  If $\alpha > 1$ then there is a sequence $\{n_k\}$ such that
  $\sqrt[n]{\abs{\vec{a}_{n_k}}}\to\alpha$. Hence
  $\abs{\vec{a}_n} > 1$ for infinitely many values of $n$ and we
  cannot have $\vec{a}_n\to\vec0$. Therefore $\sum\vec{a}_n$ diverges.

  The fact that $\sum1/n$ diverges while $\sum1/n^2$ converges shows
  that $\alpha=1$ gives no information about convergence.
\end{proof}
\begin{thm}[Ratio Test]
  The series $\sum\vec{a}_n$
  \begin{enumerate}
  \item converges if
    \begin{equation*}
      \limsup_{n\to\infty}\frac{\abs{\vec{a}_{n+1}}}
      {\abs{\vec{a}_n}} < 1,
    \end{equation*}
  \item diverges if
    \begin{equation*}
      \frac{\abs{\vec{a}_{n+1}}}{\abs{\vec{a}_n}}\geq1
      \quad\text{for all $n\geq n_0$},
    \end{equation*}
    where $n_0$ is some fixed integer.
  \end{enumerate}
\end{thm}
\begin{proof}
  If the first condition holds, then we can find $\beta<1$ and an
  integer $N$ such that
  \begin{equation*}
    \frac{\abs{\vec{a}_{n+1}}}{\abs{\vec{a}_n}}<\beta
    \quad\text{for $n\geq N$}.
  \end{equation*}
  Then
  \begin{align*}
    \abs{\vec{a}_{N+1}} &< \beta\abs{\vec{a}_N}, \\
    \abs{\vec{a}_{N+2}} &< \beta\abs{\vec{a}_{N+1}} < \beta^2\abs{\vec{a}_N}, \\
                        &\;\;\vdots \\
    \abs{\vec{a}_{N+p}} &< \beta^p\abs{\vec{a}_N}.
  \end{align*}
  So for $n\geq N$,
  \begin{equation*}
    \abs{\vec{a}_n} < \abs{\vec{a}_N}\beta^{-N}\beta^n,
  \end{equation*}
  and convergence follows from the comparison test since $\sum\beta^n$
  converges.

  In the case where $\abs{\vec{a}_{n+1}}\geq\abs{\vec{a}_n}$ for
  $n\geq n_0$, it is clear that $\vec{a}_n$ does not tend to $\vec0$
  and $\sum\vec{a}_n$ diverges.
\end{proof}
\begin{thm}
  Suppose
  \begin{enumerate}
  \item the partial sums $\vec{A}_n$ of $\sum\vec{a}_n$ form a bounded
    sequence;
  \item $b_0\geq b_1\geq b_2\geq\cdots$;
  \item $\displaystyle\lim_{n\to\infty}b_n = 0$.
  \end{enumerate}
  Then $\sum b_n\vec{a}_n$ converges.
\end{thm}
\begin{proof}
  Since $\{\vec{A}_n\}$ is bounded we can find $M$ such that
  $\abs{\vec{A}_n}\leq M$ for all $n$. Given $\varepsilon>0$ there is
  an integer $N$ such that $b_N\leq\varepsilon/2M$. For
  $N\leq p\leq q$, we have
  \begin{align*}
    \Abs{\sum_{n=p}^qb_n\vec{a}_n}
    &= \Abs{\sum_{n=p}^q(b_n - b_{n+1})\vec{A}_n
      + b_q\vec{A}_q - b_p\vec{A}_{p-1}} \\
    &\leq M\Abs{\sum_{n=p}^{q-1}(b_n - b_{n+1}) + b_q + b_p} \\
    &= 2Mb_p\leq 2Mb_N\leq\varepsilon.
  \end{align*}
  Therefore $\sum b_n\vec{a}_n$ converges by the Cauchy criterion.
\end{proof}
\begin{thm}
  If $\sum\vec{a}_n$ converges absolutely, then $\sum\vec{a}_n$
  converges.
\end{thm}
\begin{proof}
  From the triangle inequality, we have
  \begin{equation*}
    \Abs{\sum_{k=n}^m\vec{a}_k}\leq\sum_{k=n}^m\abs{\vec{a}_k},
  \end{equation*}
  so the series $\sum\vec{a}_n$ converges by the Cauchy criterion.
\end{proof}
\begin{thm}
  If $\sum\vec{a}_n = \vec{A}$, and $\sum\vec{b}_n = \vec{B}$, then
  $\sum(\vec{a}_n+\vec{b}_n) = \vec{A} + \vec{B}$, and
  $\sum c\vec{a}_n = c\vec{A}$, for any fixed $c$.
\end{thm}
\begin{proof}
  For each $n\geq0$, set
  \begin{equation*}
    \vec{A}_n = \sum_{k=0}^n\vec{a}_k
    \quad\text{and}\quad
    \vec{B}_n = \sum_{k=0}^n\vec{b}_k.
  \end{equation*}
  Then
  \begin{equation*}
    \vec{A}_n + \vec{B}_n = \sum_{k=0}^n(\vec{a}_k + \vec{b}_k)
    \quad\text{and}\quad
    c\vec{A}_n = \sum_{k=0}^nc\vec{a}_n.
  \end{equation*}
  So
  \begin{equation*}
    \lim_{n\to\infty}(\vec{A}_n + \vec{B}_n)
    = \lim_{n\to\infty}\vec{A}_n
    + \lim_{n\to\infty}\vec{B}_n = \vec{A} + \vec{B}
  \end{equation*}
  and
  \begin{equation*}
    \lim_{n\to\infty}c\vec{A}_n = c\vec{A}. \qedhere
  \end{equation*}
\end{proof}
\begin{thm}
  If $\sum\vec{a}_n$ is a series of vectors which converges
  absolutely, then every rearrangement of $\sum\vec{a}_n$ converges,
  and they all converge to the same sum.
\end{thm}
\begin{proof}
  Again, the proof is mostly identical:

  Let $\sum\vec{a}_n'$ be a rearrangement, with
  $\vec{a}_n' = \vec{a}_{k_n}$ and with partial sums
  $\vec{s}_n'$. Given $\varepsilon > 0$, there exists an integer $N$
  such that
  \begin{equation*}
    \sum_{i=n}^m\abs{\vec{a}_i}\leq\varepsilon
    \quad\text{for all $m\geq n\geq N$}.
  \end{equation*}
  Choose $p$ such that the integers $1,2,\dots,N$ are all contained
  within the set $k_1,k_2,\dots,k_p$ (where each $k_i$ is defined as
  in Definition~3.52). Then if $n>p$, the vectors
  $\vec{a}_1,\dots,\vec{a}_N$ will cancel in the difference
  $\vec{s}_n - \vec{s}_n'$ so that
  $\abs{\vec{s}_n - \vec{s}_n'}\leq\varepsilon$. Hence
  $\{\vec{s}_n'\}$ converges to the same sum as $\{\vec{s}_n\}$.
\end{proof}

\Exercise{16} Fix a positive number $\alpha$. Choose
$x_1 > \sqrt{\alpha}$, and define $x_2, x_3, x_4, \dots,$ by the
recursion formula
\begin{equation*}
  x_{n+1} = \frac12\left(x_n + \frac\alpha{x_n}\right).
\end{equation*}
\begin{enumerate}
\item Prove that $\{x_n\}$ decreases monotonically and that
  $\lim x_n = \sqrt\alpha$.
  \begin{proof}
    Since $x_1 > \sqrt\alpha$, we have $\alpha < x_1^2$. Note also
    that for each $n\geq1$,
    \begin{align*}
      x_{n+1}^2 - \alpha
      &= \frac14\left(x_n + \frac\alpha{x_n}\right)^2
        - \frac14\left(\frac{4x_n\alpha}{x_n}\right) \\
      &= \frac14\left(x_n - \frac\alpha{x_n}\right)^2 \geq 0.
    \end{align*}
    Therefore $\alpha < x_n^2$ for all $n$. So for each $n$,
    \begin{equation*}
      x_{n+1} = \frac12\left(x_n + \frac{\alpha}{x_n}\right)
      \leq x_n,
    \end{equation*}
    and we see that $\{x_n\}$ is a monotonically decreasing sequence.

    Now, since $\{x_n\}$ is a monotonic sequence that is bounded below
    by $\sqrt\alpha$, it must converge to some value $x$. Then we have
    \begin{equation*}
      x = \lim_{n\to\infty}x_n
      = \lim_{n\to\infty}\frac12\left(x_n + \frac\alpha{x_n}\right)
      = \frac12\left(x + \frac\alpha{x}\right).
    \end{equation*}
    So $2x = x + \alpha/x$ hence $x = \sqrt\alpha$.
  \end{proof}
\item Put $\varepsilon_n = x_n - \sqrt\alpha$, and show that
  \begin{equation*}
    \varepsilon_{n+1} = \frac{\varepsilon_n^2}{2x_n}
    < \frac{\varepsilon_n^2}{2\sqrt\alpha}
  \end{equation*}
  so that, setting $\beta = 2\sqrt\alpha$,
  \begin{equation*}
    \varepsilon_{n+1}<\beta\left(\frac{\varepsilon_1}\beta\right)^{2^n}
    \quad\text{($n = 1,2,3,\dots$)}.
  \end{equation*}
  \begin{proof}
    If $\varepsilon_n = x_n - \sqrt\alpha$ then we have
    \begin{align*}
      \varepsilon_{n+1}
      &= x_{n+1} - \sqrt\alpha
      = \frac12\left(x_n + \frac\alpha{x_n}\right) - \sqrt\alpha \\
      &= \frac{x_n^2 + \alpha - 2x_n\sqrt\alpha}{2x_n}
      = \frac{(x_n - \sqrt\alpha)^2}{2x_n}
      = \frac{\varepsilon_n^2}{2x_n} < \frac{\varepsilon_n^2}{2\sqrt\alpha}.
    \end{align*}
    Set $\beta = 2\sqrt\alpha$. Then a simple induction argument will
    show that
    \begin{equation*}
      \varepsilon_{n+1} < \beta\left(\frac{\varepsilon_1}\beta\right)^{2^n}
      \quad\text{for $n\geq1$}. \qedhere
    \end{equation*}
  \end{proof}
\item This is a good algorithm for computing square roots, since the
  recursion formula is simple and the convergence is extremely
  rapid. For example, if $\alpha = 3$ and $x_1 = 2$, show that
  $\varepsilon_1/\beta < 1/10$ and that therefore
  \begin{equation*}
    \varepsilon_5 < 4\cdot10^{-16},
    \quad\varepsilon_6 < 4\cdot10^{-32}.
  \end{equation*}
  \begin{proof}
    Since $\sqrt3 < 9/5$ we have
    \begin{equation*}
      \frac{\varepsilon_1}\beta = \frac{x_1-\sqrt\alpha}{2\sqrt\alpha}
      = \frac{2-\sqrt3}{2\sqrt3} = \frac{\sqrt3}3 - \frac12
      < \frac35 - \frac12 = \frac1{10}.
    \end{equation*}
    Then we have
    \begin{equation*}
      \varepsilon_5 < \beta\left(\frac1{10}\right)^{16} < 4\cdot10^{-16}
      \quad\text{and}\quad
      \varepsilon_6 < \beta\left(\frac1{10}\right)^{32} < 4\cdot10^{-32}.
      \qedhere
    \end{equation*}
  \end{proof}
\end{enumerate}

% \Exercise{17} Fix $\alpha > 1$. Take $x_1 > \sqrt\alpha$, and define
% \begin{equation*}
%   x_{n+1} = \frac{\alpha+x_n}{1+x_n}
%   = x_n + \frac{\alpha - x_n^2}{1 + x_n}.
% \end{equation*}
% \begin{enumerate}
% \item Prove that $x_1 > x_3 > x_5 > \cdots$.
%   \begin{proof}
%     If $x_n > \sqrt\alpha$ then $\alpha - x_n^2 < 0$ and
%     \begin{equation*}
%       x_{n+1} = x_n + \frac{\alpha - x_n^2}{1 + x_n} < x_n.
%     \end{equation*}
%     On the other hand, if $x_n < \sqrt\alpha$ then
%     $\alpha - x_n^2 > 0$ and we have
%     \begin{equation*}
%       x_{n+1} = x_n + \frac{\alpha - x_n^2}{1 + x_n} > x_n.
%     \end{equation*}
%     Since $x_1 > \sqrt\alpha$, it follows that $x_n > \sqrt\alpha$
%     when $n$ is odd and $x_n < \sqrt\alpha$ when $n$ is even.
%   \end{proof}
% \item Prove that $x_2 < x_4 < x_6 < \cdots$.
% \item Prove that $\lim x_n = \sqrt\alpha$.
% \item Compare the rapidity of convergence of this process with the one
%   described in Exercise~16.
% \end{enumerate}

\Exercise{19} Associate to each sequence $a = \{\alpha_n\}$, in which
$\alpha_n$ is $0$ or $2$, the real number
\begin{equation*}
  x(a) = \sum_{n=1}^\infty\frac{\alpha_n}{3^n}.
\end{equation*}
Prove that the set of all $x(a)$ is precisely the Cantor set described
in Sec.~2.44.
\begin{proof}
  $x(a)$ is the set of all real numbers in $[0,1]$ which can be
  written in ternary without using the digit $1$. But in removing the
  middle third of the interval $[0,1]$, we are removing those numbers
  whose ternary expansion must have a $1$ as the first digit (note
  that while $1/3 = 0.1_3$, we can also write $1/3 = 0.0\bar2_3$).

  And in removing the middle third of the intervals $[0,1/3]$ and
  $[2/3,1]$ we are removing those numbers whose ternary expansion has
  a $1$ in the second digit. This process continues, so that at each
  step we are removing numbers whose ternary expansions have a $1$ in
  the $n$th place. This gives an intuitive explanation for why these
  two sets are the same.
\end{proof}

\Exercise{20} Suppose $\{p_n\}$ is a Cauchy sequence in a metric space
$X$, and some subsequence $\{p_{n_i}\}$ converges to a point $p\in
X$. Prove that the full sequence $\{p_n\}$ converges to $p$.
\begin{proof}
  Given $\varepsilon > 0$, we can find $N_0$ such that
  \begin{equation*}
    d(p_{n_i},p) < \frac\varepsilon2
    \quad\text{for all $n_i\geq N_0$}.
  \end{equation*}
  Since $\{p_n\}$ is a Cauchy sequence, we can also find $N_1$ such
  that
  \begin{equation*}
    d(p_m, p_n) < \frac\varepsilon2
    \quad\text{for all $m,n\geq N_1$}.
  \end{equation*}
  Taking $N = \max(N_0,N_1)$ we have
  \begin{equation*}
    d(p_m, p) \leq d(p_m, p_{n_i}) + d(p_{n_i}, p)
    < \varepsilon
    \quad\text{for all $m,n_i\geq N$}.
  \end{equation*}
  Hence $p_n\to p$.
\end{proof}

\Exercise{21} Prove the following analogue of Theorem~3.10(b): If
$\{E_n\}$ is a sequence of closed nonempty and bounded sets in a {\em
  complete} metric space $X$, if $E_n\supset E_{n+1}$, and if
\begin{equation*}
  \lim_{n\to\infty}\diam E_n = 0,
\end{equation*}
then $\bigcap_1^\infty E_n$ consists of exactly one point.
\begin{proof}
  For each $n$, choose $x_n\in E_n$. Then $\{x_n\}$ is a Cauchy
  sequence since $E_{n+1}\subset E_n$ and $\diam E_n\to0$. Therefore
  the sequence $\{x_n\}$ must converge since $X$ is complete. Suppose
  it converges to $x$. Then $x$ is a limit point of $E_n$ for each
  $n$, so $x\in E_n$ since $E_n$ is closed. Therefore
  $x\in\bigcap E_n$. And if $y\in\bigcap E_n$, the fact that
  $\diam E_n\to0$ means that $d(x,y) < \varepsilon$ for all
  $\varepsilon > 0$, so that in fact $x = y$.
\end{proof}

\Exercise{22} Suppose $X$ is a nonempty complete metric space, and
$\{G_n\}$ is a sequence of dense open subsets of $X$. Prove Baire's
theorem, namely, that $\bigcap_1^\infty G_n$ is not empty. (In fact,
it is dense in $X$).
\begin{proof}
  Pick $x_1\in G_1$. Since $G_1$ is open, there is a neighborhood of
  $x_1$ contained entirely within $G_1$. Make the neighborhood small
  enough so that its closure is contained within $G_1$. That is, we
  find a closed ball $E_1$ centered at $x_1$ with radius $r_1$ such that
  $E_1\subset G_1$.

  Assuming that the closed ball $E_n$ centered at $x_n$ with radius
  $r_n$ has been chosen so that $E_n\subset G_n$, pick a point
  $y\in E_n$. Since $G_{n+1}$ is dense in $X$, either $y\in G_{n+1}$
  or $y$ is a limit point of $G_{n+1}$. If the former, set
  $x_{n+1} = y$. If the latter, take a neighborhood of $y$ with radius
  small enough so that the neighborhood is contained within
  $E_n$. Then this neighborhood must contain points of $G_{n+1}$, so
  choose one and call it $x_{n+1}$. Now take a closed ball $E_{n+1}$
  centered at $x_{n+1}$ and make its radius $r_{n+1}$ small enough so
  that it is contained within $E_n\cap G_{n+1}$.

  By construction, each $E_n$ is a closed nonempty and bounded set
  with $E_1\supset E_2\supset E_3\supset\cdots$. Moreover, since
  $r_n\to0$ we have $\lim\diam E_n = 0$ as well. Since $X$ is
  complete, we know by the previous exercise that
  $\bigcap_1^\infty E_n$ is nonempty. But each $E_n\subset G_n$, so
  this implies that $\bigcap_1^\infty G_n$ is nonempty as well.
\end{proof}

\Exercise{23} Suppose $\{p_n\}$ and $\{q_n\}$ are Cauchy sequences in
a metric space $X$. Show that the sequence $\{d(p_n,q_n)\}$ converges.
\begin{proof}
  Given $\varepsilon > 0$ choose $N$ large enough so that
  \begin{equation*}
    d(p_n, p_m) < \frac\varepsilon2
    \quad\text{and}\quad
    d(q_n, q_m) < \frac\varepsilon2
    \quad\text{for all $m,n\geq N$}.
  \end{equation*}
  Since for any $m,n$
  \begin{equation*}
    d(p_n,q_n) \leq d(p_n,p_m)+d(p_m,q_m)+d(q_m,q_n),
  \end{equation*}
  we have for $m,n\geq N$ that
  \begin{equation*}
    \abs{d(p_n,q_n) - d(p_m,q_m)} \leq d(p_n,p_m) + d(q_m,q_n)
    < \varepsilon.
  \end{equation*}
  Therefore the sequence $\{d(p_n,q_n)\}$ is Cauchy. Any Cauchy
  sequence of real numbers must converge ($R^1$ is complete), so
  $\{d(p_n,q_n)\}$ converges.
\end{proof}

\Exercise{24} Let $X$ be a metric space.
\begin{enumerate}
\item Call two Cauchy sequences $\{p_n\}$, $\{q_n\}$ in $X$ {\em
    equivalent} if
  \begin{equation*}
    \lim_{n\to\infty} d(p_n,q_n) = 0.
  \end{equation*}
  Prove that this is an equivalence relation.
  \begin{proof}
    Denote the relation by $\sim$. Clearly $\{p_n\}$ is equivalent to
    itself, since $d(p_n,p_n) = 0$ for all $n$, so $\sim$ is
    reflexive. Since $d(p_n,q_n) = d(q_n,p_n)$ we also have that
    $\sim$ is symmetric.

    Now suppose that $\{p_n\}$, $\{q_n\}$, and $\{r_n\}$ are sequences
    with $\{p_n\}$ equivalent to $\{q_n\}$ and $\{q_n\}$ equivalent to
    $\{r_n\}$. Since
    \begin{equation*}
      d(p_n,r_n) \leq d(p_n,q_n) + d(q_n,r_n),
    \end{equation*}
    it follows that
    \begin{equation*}
      \lim_{n\to\infty}d(p_n,r_n) = 0,
    \end{equation*}
    so $\{p_n\}$ is equivalent to $\{r_n\}$ and $\sim$ is
    transitive. This shows that $\sim$ is an equivalence relation.
  \end{proof}
\item Let $X^*$ be the set of all equivalence classes so obtained. If
  $P\in X^*$, $Q\in X^*$, $\{p_n\}\in P$, $\{q_n\}\in Q$, define
  \begin{equation*}
    \Delta(P, Q) = \lim_{n\to\infty}d(p_n,q_n);
  \end{equation*}
  by Exercise~23, this limit exists. Show that the number
  $\Delta(P, Q)$ is unchanged if $\{p_n\}$ and $\{q_n\}$ are replaced
  by equivalent sequences, and hence that $\Delta$ is a distance
  function in $X^*$.
  \begin{proof}
    Suppose $\{p_n'\}$ is any sequence equivalent to $\{p_n\}$ and
    $\{q_n'\}$ is any sequence equivalent to $\{q_n\}$. Then
    \begin{equation*}
      d(p_n,q_n) \leq d(p_n,p_n') + d(p_n',q_n') + d(q_n',q_n)
    \end{equation*}
    so
    \begin{equation*}
      \abs{d(p_n,q_n) - d(p_n',q_n')} \leq d(p_n,p_n') + d(q_n',q_n).
    \end{equation*}
    Since the right-hand side of this inequality can be made
    arbitrarily small by choosing large enough $n$, it follows that
    \begin{equation*}
      \lim_{n\to\infty}(d(p_n,q_n) - d(p_n',q_n')) = 0
    \end{equation*}
    or
    \begin{equation*}
      \lim_{n\to\infty}d(p_n,q_n) = \lim_{n\to\infty}d(p_n',q_n').
    \end{equation*}
    Therefore $\Delta$ is a well-defined function from $X^*\times X^*$
    into $R^1$.

    It is evident that $\Delta(P,P) = 0$ and
    $\Delta(P,Q) = \Delta(Q,P)$. And the triangle inequality follows
    from the triangle inequality in $X$. Hence $\Delta$ is a distance
    function.
  \end{proof}
\item Prove that the resulting metric space $X^*$ is complete.
  \begin{proof}
    Let $\{P_k\}$ be a Cauchy sequence in $X^*$ and for each $k$, let
    $\{p_{k,n}\}$ be a Cauchy sequence in $P_k$. Choose a positive
    integer $N_k$ such that
    \begin{equation*}
      d(p_{k,n},p_{k,m}) < \frac1{2^k}
      \quad\text{for all $m,n\geq N_k$}.
    \end{equation*}
    Now set $q_k = p_{k,N_k}$. Then
    \begin{equation}
      \label{eq:d-q-k-to-p-k-n-less-than-2-to-the-minus-k}
      d(q_k,p_{k,n}) < \frac1{2^k}
      \quad\text{for all $n\geq N_k$}.
    \end{equation}

    Since
    \begin{equation*}
      \Delta(P_i,P_j) = \lim_{n\to\infty}d(p_{i,n},p_{j,n}),
    \end{equation*}
    we can also find $M(i,j)$ so that
    \begin{equation*}
      \abs{\Delta(P_i,P_j) - d(p_{i,n},p_{j,n})} < \frac1{2^{\max(i,j)}}
      \quad\text{for all $n\geq M(i,j)$}.
    \end{equation*}

    Now, for any $i,j,n$, we have
    \begin{equation*}
      d(q_i,q_j) \leq d(q_i,p_{i,n}) + d(p_{i,n},p_{j,n}) + d(p_{j,n},q_j).
    \end{equation*}
    Therefore,
    \begin{equation*}
      d(q_i,q_j)\leq \frac1{2^i}
      + \left(\Delta(P_i,P_j) + \frac1{2^{\max(i,j)}}\right)
      + \frac1{2^j}
    \end{equation*}
    for all $n\geq\max(N_i,N_j,M(i,j))$. Now $d(q_i,q_j)$ can be made
    arbitrarily small for sufficiently large $i$ and $j$, so $\{q_k\}$
    is a Cauchy sequence.

    Let $P$ be the equivalence class in $X^*$ containing
    $\{q_k\}$. Then
    \begin{equation*}
      \Delta(P_k,P) = \lim_{n\to\infty}d(p_{k,n},q_n)
      \leq \lim_{n\to\infty}(d(p_{k,n},q_k) + d(q_k,q_n)).
    \end{equation*}
    By \eqref{eq:d-q-k-to-p-k-n-less-than-2-to-the-minus-k} and the
    fact that $\{q_k\}$ is Cauchy, we have
    \begin{equation*}
      \lim_{k\to\infty}P_k = P.
    \end{equation*}
    This shows that $X^*$ is complete.
  \end{proof}
\item For each $p\in X$, there is a Cauchy sequence all of whose terms
  are $p$; let $P_p$ be the element of $X^*$ which contains this
  sequence. Prove that
  \begin{equation*}
    \Delta(P_p,P_q) = d(p,q)
  \end{equation*}
  for all $p,q\in X$. In other words, the mapping $\varphi$ defined by
  $\varphi(p) = P_p$ is an isometry (i.e., a distance-preserving
  mapping) of $X$ into $X^*$.
  \begin{proof}
    This result is immediate:
    \begin{equation*}
      \Delta(P_p,P_q) = \lim_{n\to\infty}d(p,q) = d(p,q).
    \end{equation*}
    Therefore $\varphi$ preserves distance.
  \end{proof}
\item Prove that $\varphi(X)$ is dense in $X^*$, and that
  $\varphi(X) = X^*$ if $X$ is complete. By (d), we may identify $X$
  and $\varphi(X)$ and thus regard $X$ as embedded in the complete
  metric space $X^*$. We call $X^*$ the {\em completion} of $X$.
  \begin{proof}
    Let $P\in X^*$. If $P\in\varphi(X)$ then we are done, so suppose
    $P\not\in\varphi(X)$. Then we need to show that $P$ is a limit
    point of $\varphi(X)$. Given $\varepsilon > 0$, let $\{p_n\}\in P$
    and choose $N$ so that
    \begin{equation*}
      d(p_n,p_m) < \varepsilon
      \quad\text{for all $n,m\geq N$}.
    \end{equation*}
    Then
    \begin{equation*}
      \Delta(P, P_{p_N}) = \lim_{n\to\infty}d(p_n,p_N) \leq \varepsilon.
    \end{equation*}
    Since $P_{p_N} = \varphi(p_N)$, this shows that any neighborhood
    of $P$ in $X^*$ contains a point from $\varphi(X)$, so that
    $\varphi(X)$ is dense in $X^*$.

    Lastly, if $X$ is complete, then for each $P\in X^*$ and for each
    $\{p_n\}\in P$, $\{p_n\}$ must converge to a point $p$ in $X$. Any
    sequence equivalent to $\{p_n\}$ will also converge to $p$, so
    $P = P_p = \varphi(p)$. Therefore $\varphi(X) = X^*$.
  \end{proof}
\end{enumerate}

\Exercise{25} Let $X$ be the metric space whose points are the
rational numbers, with the metric $d(x,y) = \abs{x - y}$. What is the
completion of this space?
\begin{solution}
  Every Cauchy sequence in $X$ converges to a unique real number, and
  two sequences in $X$ are equivalent if and only if they converge to
  the same real number. So the completion of $X$ is the real numbers.
\end{solution}
